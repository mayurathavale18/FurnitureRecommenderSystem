{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83qCXrUPHows"
   },
   "source": [
    "# LDA-based Recommender System\n",
    "LDA is being often described as the simplest topic model (Blei and Lafferty, 2009): The intuition behind this model is that documents exhibit multiple topics.\n",
    "\n",
    "The aim of topic modelling is to automatically discover the topics from a collection of documents, which are observed, while the topic structure is hidden.\n",
    "\n",
    "Rather than simply concatenating image and text features and\n",
    "linearly combining them, PolyLDA enables us to learn two distinct\n",
    "but coupled latent style representations. This allows for a versatile\n",
    "interpretation of style. We then use these learned styles to make\n",
    "bundle or assortment recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaFcYg2-z3u2",
    "tags": []
   },
   "source": [
    "## Reference\n",
    "0. LDA pacakge: https://github.com/lda-project/lda\n",
    "1. LDA-based book recommender system: https://humboldt-wi.github.io/blog/research/information_systems_1819/is_lda_final/\n",
    "2. A Multimodal Recommender System for Large-scale Assortment Generation in E-commerce: https://arxiv.org/abs/1806.11226\n",
    "3. Discovering Style Trends through Deep Visually Aware Latent Item Embeddings: https://arxiv.org/abs/1804.08704\n",
    "4. Text LDA code demo, https://github.com/agrawal-priyank/machine-learning-clustering-retrieval/blob/master/latent-dirichlet-allocation/latent-dirichlet-allocation.ipynb\n",
    "5. Build image recommendation app: https://towardsdatascience.com/a-flask-app-for-image-recommendations-a865e1496a0d, \n",
    "https://towardsdatascience.com/image-recommendations-with-pytorch-flask-postgresql-heroku-deployment-206682d06c6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for raw data\n",
    "! Due to the scraping code, the downloaded images are more than records in csv, so only left those that exist in both parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "ROOT = os.environ.get(\"ROOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_path = ROOT + '/Meta.nosync'\n",
    "model_path = ROOT + '/Model'\n",
    "static_model_path = ROOT + '/Engine/imageRecommender/static/pickles'\n",
    "static_image_path = ROOT + '/Engine/imageRecommender/static/site_imgs/images'\n",
    "CLS = ['chairs-chaises', 'coffee-tables', 'ottomans-benches', 'tv-stands-tv-mounts', \n",
    "       'recliners', 'sofa-console-tables', 'sofas', 'end-accent-tables', 'cabinets-shelvings']\n",
    "NUM_CLASSES = 6\n",
    "RANDOM_CROP_SIZE = 64\n",
    "TARGET_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.01\n",
    "SEED = 1\n",
    "PLOT_SIZE_1 = (40, 40)\n",
    "PLOT_SIZE_2 = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = pd.read_csv(Path(ROOT, 'Furniture.csv'), index_col=0)\n",
    "df_org['keep'] = 0 # 1 means to keep the record, otherwises abandon\n",
    "df_org['img_abs_path'] = None # add local image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove existing folder\n",
    "if os.path.exists(meta_path):\n",
    "    shutil.rmtree(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# freq of each type\n",
    "df_org['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "types_remove = ['living-room-packages', 'sectionals', 'loveseats']\n",
    "img_abs_path = glob(ROOT + '/Meta_org/*/*')\n",
    "\n",
    "# create new dir\n",
    "if not os.path.exists(meta_path):\n",
    "    os.makedirs(meta_path)\n",
    "for i in df_org['category'].unique():\n",
    "    if i not in types_remove: # remove types that are not required for this study\n",
    "        if not os.path.exists(Path(meta_path, i)):\n",
    "            os.makedirs(Path(meta_path, i))\n",
    "\n",
    "# copy image to new folder\n",
    "for i in img_abs_path:\n",
    "    main_path = i.rsplit('/', 3)[0]\n",
    "    subfolder_name = i.rsplit('/', 3)[2]\n",
    "    file_name = i.rsplit('/', 3)[3]\n",
    "    file_name_1 = file_name.split('.')[0]\n",
    "    if subfolder_name not in types_remove and file_name_1 in df_org['name'].tolist():\n",
    "        print('image exist in csv so move it to meta folder')\n",
    "        shutil.copy(i, Path(main_path, 'Meta.nosync', subfolder_name))\n",
    "        df_org.loc[df_org['name'] == file_name_1, 'keep'] = 1\n",
    "        df_org.loc[df_org['name'] == file_name_1, 'img_abs_path'] = str(Path(main_path, 'Meta.nosync', subfolder_name, file_name_1 + '.jpg'))\n",
    "    else:\n",
    "        print(i, 'is not requried or not match with the record in csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_0 = df_org[df_org['keep'] == 1]\n",
    "print('number of records left are', len(df_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(glob(meta_path + '/*/*')) == len(df_0), 'csv record does not match with image record, need to troubleshoot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # if above has error, execute below\n",
    "# # manually process unmatched records, e.g. delete imgs from meta folder\n",
    "# img_abs_path = glob(meta_path + '/*/*')\n",
    "# for index, row in df_0.iterrows():\n",
    "#     try:\n",
    "#         idx = [i for i, s in enumerate(img_abs_path) if row['name'] + '.jpg' in s]\n",
    "#         if len(idx) > 1:\n",
    "#             print(idx)\n",
    "#             print(np.array(img_abs_path)[idx])\n",
    "#             print(df.loc[df['name'] == row['name'], ['category', 'name']])\n",
    "#     except Exception as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # manually process unmatched types\n",
    "# img_abs_path = glob(meta_path + '/*/*')\n",
    "# for index, row in df_0.iterrows():\n",
    "#     for i in img_abs_path:\n",
    "#         if row['name'] + '.jpg' == i.rsplit('/')[-1]:\n",
    "#             assert row['category'] == i.rsplit('/')[-2], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove image that has no white background\n",
    "def find_white_background(img_abs_path, threshold=0.01):\n",
    "    # remove images with transparent or white background\n",
    "    imgArr = img_to_array(load_img(img_abs_path, target_size = (TARGET_SIZE, TARGET_SIZE)))\n",
    "    background = np.array([255, 255, 255])\n",
    "    percent = (imgArr == background).sum() / imgArr.size\n",
    "    # print('white background percent is {:.2f}'.format(percent))\n",
    "    if percent >= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "wb = df_0['img_abs_path'].apply(lambda x: find_white_background(x))\n",
    "wb_idx = wb[wb == True]\n",
    "print('No. of pics that have no white groud is', len(wb[wb != True]), ', remove them from the study.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1 = df_0.loc[wb_idx.index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from glob import glob\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "# import enchant\n",
    "# from nltk.tokenize import word_tokenize, pos_tag\n",
    "# The NLTK Lemmatization method is based on WordNetâ€™s built-in morph function.\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import lda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1 = df_1.reset_index(drop=True)\n",
    "# shuffle dataset\n",
    "df = df_1.sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# freq of each type\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img(img_abs_path):\n",
    "    try:\n",
    "        original = load_img(img_abs_path)\n",
    "        plt.figure(figsize=[7,7])\n",
    "        plt.title(img_abs_path)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(original)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_cleaner(lst: list) -> str:\n",
    "    cleaned_lst = []\n",
    "    for a, i in enumerate(lst):\n",
    "        # print(a, i)\n",
    "        # only extract informative tags # cover, frame\n",
    "        if [w for w in ['finish', 'material', 'cover'] if w in i.lower()]:\n",
    "            # split by :\n",
    "            lst2 = i.split(':')\n",
    "            for j in lst2:\n",
    "                # should always come with capital and small letters or precentage and small letters\n",
    "                lst3 = re.findall('(?:[0-9]|[A-Z])[^A-Z]*', j)\n",
    "                for k in lst3:\n",
    "                    # remove string that starts with ' b'\n",
    "                    if not k.startswith((\"[\", \" b\")):\n",
    "                        q = re.sub(\"[^\\sA-Za-z]\", ' ', k).lstrip(' ')\n",
    "                        cleaned_lst.append(q)\n",
    "    cleaned = ' '.join(map(str, cleaned_lst))\n",
    "    cleaned = ' '.join(cleaned.split()) # compress extra white space\n",
    "    return cleaned\n",
    "\n",
    "# print(df['attributes'][2])\n",
    "# print(text_cleaner(df['attr_1'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# d = enchant.Dict(\"en_US\")\n",
    "d = words.words()\n",
    "def text_lemmatizer(s: str) -> str:\n",
    "    token_words = word_tokenize(s) \n",
    "    # exact noun and adj only. TODO: consider single and plural the same\n",
    "    help_words = [w[0] for w in (pos_tag(token_words)) if ('NN' in w[1]) or ('JJ' in w[1])]\n",
    "    help_words_2 = []\n",
    "    for word in help_words:\n",
    "        # exact English word only, however, word like 'microsuede' will be ignored\n",
    "        if len(word) > 3:\n",
    "            lemword = WordNetLemmatizer().lemmatize(word.lower())\n",
    "            help_words_2.append(lemword)\n",
    "    cleaned = ' '.join(map(str, help_words_2))\n",
    "    return cleaned\n",
    "\n",
    "# print(df['name'][12])\n",
    "# print(df['attr_1'][12])\n",
    "# print(df['attr_2'][12])\n",
    "# text_lemmatizer(df['attr_2'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_remover(s: str) -> str:\n",
    "    token_words = word_tokenize(s) \n",
    "    cleaned_lst = []\n",
    "    for w in token_words:\n",
    "        # remove frequent non-helpful words\n",
    "        if w.lower() not in ['finish', 'material', 'materials', 'cover', 'frame', 'fabric', 'content', 'unique', 'feature', 'features', 'provide', 'ensure',\n",
    "                     'require', 'create', 'look', 'use', 'color', 'fabric', 'colour', 'warranty', 'year', 'key', 'construction', 'furniture']:\n",
    "            cleaned_lst.append(w)\n",
    "    cleaned = ' '.join(map(str, cleaned_lst))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot image based on its position in the file\n",
    "idx = 0\n",
    "img_path = df.iloc[idx]['img_abs_path']\n",
    "plot_img(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot image based on its position in the file\n",
    "idx = 100\n",
    "img_name = df.iloc[idx]['img_abs_path']\n",
    "plot_img(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split words and remove all white blanks\n",
    "df['attr_1'] = df['details'].apply(lambda x: list(filter(None, x.split(','))))\n",
    "# remove all words that contain special chars\n",
    "df['attr_2'] = df['attr_1'].apply(lambda x: text_cleaner(x))\n",
    "#  lemmatization\n",
    "df['attr_3'] = df['attr_2'].apply(lambda x: text_lemmatizer(x))\n",
    "#  remove most frequet non-meaninful words\n",
    "df['attr_4'] = df['attr_3'].apply(lambda x: text_remover(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, df['name'].iloc[i], '----->', df['attr_2'].iloc[i], '----->', df['attr_3'].iloc[i], '----->', df['attr_4'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize most frequent words, handcrafted excluded in text_remover()\n",
    "all_tokens = []\n",
    "for i in df.itertuples():\n",
    "    all_tokens.extend(word_tokenize(i[-2])) # df['attr_3']\n",
    "\n",
    "counter=collections.Counter(all_tokens)\n",
    "result = counter.most_common(50)\n",
    "mc = pd.DataFrame(result, columns = ['Word', 'Count'])\n",
    "mc.plot.bar(x='Word',y='Count', figsize=(20,20))\n",
    "# decide to remove all words up to 'Leather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of word\n",
    "concept: https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_corpus = df['attr_4'].tolist() # if use attr_2, 2161 vocabs\n",
    "vectorizer = CountVectorizer(stop_words='english', analyzer='word', lowercase=True)\n",
    "X_vect = vectorizer.fit_transform(text_corpus)\n",
    "vocab = tuple(vectorizer.get_feature_names())\n",
    "X = X_vect.toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "X: array-like (samples, n_features)\n",
    "vocab: tuple of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class lda_model():\n",
    "    def __init__(self, X, corpus, vocab, n_topics, n_iter, n_top_docs=10, n_top_words=20, random_state=1):\n",
    "        self.X = X\n",
    "        self.corpus = corpus\n",
    "        self.vocab = vocab\n",
    "        self.n_topics = n_topics\n",
    "        self.n_iter = n_iter\n",
    "        self.n_top_docs = n_top_docs\n",
    "        self.n_top_words = n_top_words\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard_similarity(list1, list2):\n",
    "        s1 = set(list1)\n",
    "        s2 = set(list2)\n",
    "        return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n",
    "    \n",
    "    def fit(self):\n",
    "        # model = LatentDirichletAllocation(n_components=n_topics, max_iter=n_iter, random_state=random_state, n_jobs=-1) # max_iterï¼Œ max_doc_update_iter\n",
    "        self.model = lda.LDA(n_topics=self.n_topics, n_iter=self.n_iter, random_state=self.random_state)\n",
    "        self.model.fit(self.X)  # model.fit_transform(X) is also available\n",
    "        return self.model\n",
    "    \n",
    "    def topic_path(self, df):\n",
    "        doc_topic = self.model.transform(self.X)\n",
    "        \n",
    "        # assign topics\n",
    "        max_topic_idx = np.argmax(doc_topic, axis=1)\n",
    "        df['topic'] = max_topic_idx\n",
    "        # topic_path = []\n",
    "        for i in range(self.n_topics):\n",
    "            doc_idx = np.where(max_topic_idx == i)[0]\n",
    "            vars()['topic_' + str(i) + '_path'] = df['img_abs_path'].iloc[doc_idx].tolist()\n",
    "            vars()['topic_' + str(i) + '_attr'] = np.array(self.corpus)[doc_idx].tolist()\n",
    "            vars()['topic_' + str(i) + '_attr'] = list(' '.join(vars()['topic_' + str(i) + '_attr']).split(' ')) # convert big string to list\n",
    "            # topic_path.append(vars()['topic_' + str(i) + '_path'])\n",
    "            \n",
    "        # calculate Jaccard similarity\n",
    "        sims = []\n",
    "        for i in range(self.n_topics):\n",
    "            for j in range(i+1, self.n_topics):\n",
    "                sim = self.jaccard_similarity(vars()['topic_' + str(i) + '_attr'], vars()['topic_' + str(j) + '_attr'])\n",
    "                sims.append(sim)\n",
    "        sim_score = sum(sims) / len(sims)\n",
    "        print('mean of jaccard similarity is {:.2f}'.format(sim_score))\n",
    "        \n",
    "        # self.topic_path = dict(zip(['topic_' + str(i) + '_path' for i in range(self.n_topics)], topic_path))\n",
    "        return sim_score # , self.topic_path # {'topic_' + str(i) + '_path': vars()['topic_' + str(i) + '_path'] for i in range(self.n_topics)}\n",
    "    \n",
    "    def topic_representation(self):\n",
    "        # get word prob per topic\n",
    "        topic_word = self.model.components_\n",
    "    \n",
    "        i = 0\n",
    "        while i < self.n_topics:\n",
    "            topic_words = np.array(self.vocab)[np.argsort(topic_word[i])][:-self.n_top_words:-1]\n",
    "            print('Topic {} has {} documents'.format(i, len(df[df['topic'] == i])))\n",
    "            print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "            print('-------------List below is the top n furnitures for topic %s-----------------' %i)\n",
    "            plt.figure(figsize = PLOT_SIZE_1)\n",
    "            for j in range(self.n_top_docs):\n",
    "                try:\n",
    "                    img_path = df.loc[df['topic'] == i, 'img_abs_path'].iloc[j]\n",
    "                    ax = plt.subplot(1, self.n_top_docs, j+1)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "                    # plot filter channel in grayscale\n",
    "                    plt.imshow(load_img(img_path, target_size=(TARGET_SIZE, TARGET_SIZE)))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            plt.show()\n",
    "            i += 1\n",
    "        return None\n",
    "    \n",
    "# model = lda_model(X, multi_corpus, vocab, n_topics=6, n_iter=100)\n",
    "# model.fit()\n",
    "# sim_score = model.topic_path(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine optimal topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_TOPICS = [2, 3, 4, 5, 6, 7, 8]\n",
    "N_ITER = 1000\n",
    "RAND_STATE = 1\n",
    "N_TOP_WORDS = 10\n",
    "N_TOP_DOCS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    sim_scores = []\n",
    "    for n in N_TOPICS:\n",
    "        model = lda_model(X, text_corpus, vocab, n_topics=n, n_iter=N_ITER)\n",
    "        model.fit()\n",
    "        sim_score = model.topic_path(df)\n",
    "        sim_scores.append(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    plt.figure()\n",
    "    plt.plot(N_TOPICS, sim_scores, label='Average Overlap Between Topics', linewidth=2, markersize=12)\n",
    "    plt.xlabel('Number of topics')\n",
    "    plt.ylabel('Average Jaccard similarity')   \n",
    "    plt.title('Average Jaccard Similarity Between Topics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = lda_model(X, text_corpus, vocab, n_topics=6, n_iter=1000)\n",
    "model.fit()\n",
    "sim_score = model.topic_path(df)\n",
    "model.topic_representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_to_labels(v):\n",
    "    idx = np.where(v==1)[0][0]\n",
    "    if idx==0:\n",
    "        l='chairs'\n",
    "    if idx==1:\n",
    "        l='coffeetables'\n",
    "    if idx==2:\n",
    "        l='ottman & benches'\n",
    "    if idx==3:\n",
    "        l='TV stands & mounts'\n",
    "    if idx==4:\n",
    "        l='recliners'\n",
    "    if idx==5:\n",
    "        l='sofa & console tables'\n",
    "    if idx==6:\n",
    "        l='sofas'\n",
    "    if idx==7:\n",
    "        l='end & accent tables'\n",
    "    if idx==8:\n",
    "        l='cabinets & shelvings'\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plots(imgs, figsize=(60,30), rows=1, interp=False, titles=None):\n",
    "    if type(imgs[0]) is np.ndarray:\n",
    "        imgs = np.array(imgs).astype(np.uint8)\n",
    "        if (imgs.shape[-1] !=3 ):\n",
    "            imgs = imgs.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(imgs)//rows if len(imgs)%2 == 0 else len(imgs)//rows+1\n",
    "    for i in range(len(imgs)):\n",
    "        sp = f.add_subplot(rows,cols,i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(map_to_labels(titles[i]),fontsize=16)\n",
    "        plt.imshow(imgs[i],interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random cropping in ImageDataGenerator \n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1, )\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class image_representation():\n",
    "    def __init__(self, img_abs_path, img_size, model, filters, thres):\n",
    "        self.img_abs_path = img_abs_path\n",
    "        self.img_size = img_size\n",
    "        self.model = model\n",
    "        self.filters = filters\n",
    "        self.thres = thres\n",
    "        \n",
    "    def load_image(self, visualize=False):\n",
    "        original = load_img(self.img_abs_path, target_size=(self.img_size, self.img_size))\n",
    "        if visualize:\n",
    "            plt.figure(figsize=PLOT_SIZE_2)\n",
    "            plt.title(self.img_abs_path)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(original)\n",
    "            plt.show()\n",
    "        return original\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_filters(filters):\n",
    "        # plot first few filters, now only plot filters as 2-dimensional\n",
    "        square, ix = int(math.sqrt(filters.shape[-1])), 1\n",
    "        plt.figure(figsize=PLOT_SIZE_2) \n",
    "        # plot all 64 features in an 8x8 squares\n",
    "        ix = 1\n",
    "        for _ in range(square):\n",
    "            for _ in range(square):\n",
    "                # specify subplot and turn of axis\n",
    "                ax = plt.subplot(square, square, ix)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                # plot filter channel in grayscale\n",
    "                plt.imshow(filters[:, :, 0, ix-1], cmap='gray')\n",
    "                ix += 1\n",
    "        # show the figure\n",
    "        plt.show()\n",
    "    \n",
    "    def exact_feature_maps(self, visualize):\n",
    "        original = self.load_image(visualize=visualize)\n",
    "        img = img_to_array(original)\n",
    "        # expand dimensions so that it represents a single 'sample', i.e. 3 dims -> 4 dims\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # NOTE: self.model.predict(img) does not work in tf2.4.0rc for some reason, so use model() instead, because it returns tensor so convert to numpy\n",
    "        self.feature_maps = self.model(img).numpy()\n",
    "        \n",
    "        if visualize:\n",
    "            square, ix = int(math.sqrt(self.filters.shape[-1])), 1\n",
    "            plt.figure(figsize=PLOT_SIZE_2) \n",
    "            # plot all 64 features in an 8x8 squares\n",
    "            ix = 1\n",
    "            for _ in range(square):\n",
    "                for _ in range(square):\n",
    "                    # specify subplot and turn of axis\n",
    "                    ax = plt.subplot(square, square, ix)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "                    # plot filter channel in grayscale\n",
    "                    plt.title(\"%i\" %(ix-1))\n",
    "                    plt.imshow(self.feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "                    ix += 1\n",
    "            # show the figure\n",
    "            plt.show()\n",
    "        return self.feature_maps\n",
    "                \n",
    "    def exact_active_channels(self): \n",
    "        self.act_idx = []\n",
    "        act_num = 0\n",
    "        max_thres = self.feature_maps[0, :, :, :].max()\n",
    "        for idx in range(self.filters.shape[-1]):\n",
    "            fmap_1 = abs(self.feature_maps[0, :, :, idx]).flatten()\n",
    "            act = fmap_1[np.where(fmap_1 >= self.thres)]\n",
    "            if len(act) > np.ceil(self.feature_maps[0, :, :, :].shape[-1] / 3):\n",
    "                self.act_idx.append(idx)\n",
    "                act_num+=1          \n",
    "#             if any(i >= max_thres for i in abs(fmap_1)):\n",
    "#                 print('strongest activation is at index: ', idx)\n",
    "#         print('total number of active channels is', act_num, 'out of', self.filters.shape[-1])\n",
    "#         print('active index is', self.act_idx)\n",
    "        return self.act_idx\n",
    "        \n",
    "    def create_bovw(self, abbv):\n",
    "        act_words = [abbv + '_' + str(i) for i in self.act_idx]\n",
    "        return act_words\n",
    "    \n",
    "# img_abs_path = glob(meta_path + '/*/*')\n",
    "# for i in img_abs_path[:1]:\n",
    "#     print('-------------------------------------')\n",
    "#     print(i)\n",
    "#     img_rep = image_representation(img_abs_path=i, img_size=TARGET_SIZE, model=model_54, filters=filters_54, thres=thres_54)\n",
    "#     fmaps = img_rep.exact_feature_maps(visualize=True)\n",
    "#     print(img_rep.exact_active_channels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find threshold per layer to decide which feature map is active\n",
    "def calculate_threshold(model, imgs_num = 1000, target_size=TARGET_SIZE):\n",
    "    img_abs_path = glob(meta_path + '/*/*')\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(img_abs_path)\n",
    "\n",
    "    imgs = []\n",
    "    for i in img_abs_path[:imgs_num]:\n",
    "        original = load_img(i, target_size=(target_size, target_size))\n",
    "        # convert the image to an array\n",
    "        img = img_to_array(original)\n",
    "        # expand dimensions so that it represents a single 'sample', i.e. 3 dims -> 4 dims\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img /= 255.\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.vstack(imgs)\n",
    "    feature_maps = np.array(model(imgs))\n",
    "    print('feature maps shape is', feature_maps.shape) # (4, M, dim, dim, channel)\n",
    "    thres = np.percentile(abs(feature_maps[:, :, :, :]), 99)\n",
    "    return thres\n",
    "\n",
    "# calculate_threshold(model=model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = ImageDataGenerator(validation_split=0) # split to train and test set\n",
    "  \n",
    "# create data generator\n",
    "# class names must match names of subdirectories\n",
    "train_generator = generator.flow_from_directory(meta_path, target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "                                                        classes=CLS,\n",
    "                                                         batch_size=BATCH_SIZE, class_mode='categorical', subset='training',\n",
    "                                                         shuffle=True, seed=SEED)  # The validation data is then picked as the last 10% \n",
    "val_generator = generator.flow_from_directory(meta_path, target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "                                                        classes=CLS,\n",
    "                                                         batch_size=BATCH_SIZE, class_mode='categorical', subset='validation',\n",
    "                                                         shuffle=True, seed=SEED)  # in order to get val file name, shuffle needs to be True\n",
    "# random crop\n",
    "train_crop_generator = crop_generator(train_generator, RANDOM_CROP_SIZE)\n",
    "val_crop_generator = crop_generator(val_generator, RANDOM_CROP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(train_generator)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pre-trained resnet50 model\n",
    "model = ResNet50(weights='imagenet', include_top=False, input_shape=(TARGET_SIZE, TARGET_SIZE, 3)) # include_top=True includes fully-connected layer\n",
    "print(model.summary())\n",
    "\n",
    "filters_name = ['conv1_conv', 'conv2_block1_2_conv', 'conv2_block2_2_conv', 'conv2_block3_2_conv',\n",
    "               'conv3_block1_2_conv', 'conv3_block2_2_conv', 'conv3_block3_2_conv', 'conv3_block4_2_conv',\n",
    "               'conv4_block1_2_conv', 'conv4_block2_2_conv', 'conv4_block3_2_conv', 'conv4_block4_2_conv', 'conv4_block5_2_conv', 'conv4_block6_2_conv',\n",
    "               'conv5_block1_2_conv', 'conv5_block2_2_conv', 'conv5_block3_2_conv']\n",
    "# summarize filter shapes\n",
    "print('index, layer_name, filter_shape is as below')\n",
    "for i, layer in enumerate(model.layers):\n",
    "    # check for convolutional layer\n",
    "    if layer.name in filters_name:\n",
    "        filters = model.get_layer(layer.name).get_weights()[0]\n",
    "        print(i, layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# redefine model to output of convolutional layer, before bn\n",
    "for l in [2, 10, 22, 32, 42, 54, 64, 74, 84]:\n",
    "    vars()['model_' + str(l)] = Model(inputs=model.input, outputs=model.layers[l].output) \n",
    "    vars()['filters_' + str(l)], vars()['biases_' + str(l)] = model.layers[l].get_weights() # Creates a model that will return these outputs, given the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # run this cell at first time, then you can print out the result and run the next cell instead going forward to save time\n",
    "# # 2, 10, 22, 32, 42, 54, 64, 74, 84, 96\n",
    "# thres_2 = calculate_threshold(model=model_2)\n",
    "# thres_10 = calculate_threshold(model=model_10)\n",
    "# thres_22 = calculate_threshold(model=model_22)\n",
    "# thres_32 = calculate_threshold(model=model_32)\n",
    "# thres_42 = calculate_threshold(model=model_42)\n",
    "# thres_54 = calculate_threshold(model=model_54)\n",
    "# thres_64 = calculate_threshold(model=model_64)\n",
    "# thres_74 = calculate_threshold(model=model_74)\n",
    "# thres_84 = calculate_threshold(model=model_84)\n",
    "# print(thres_2, thres_10, thres_22, thres_32, thres_42, thres_54, thres_64, thres_74, thres_84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thres_2, thres_10, thres_22, thres_32, thres_42, thres_54, thres_64, thres_74, thres_84 =  3.640817880630493, 5.392679691314697, 1.8942697048187256, 1.2156983613967896, 4.521170139312744, 1.711417324543003, 1.8478505671024354, 2.2812237882614212, 3.172137269973753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot filters, should be 3-dims but only enable plot 2-dims\n",
    "image_representation.visualize_filters(filters=filters_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot feature maps\n",
    "img_abs_path = glob(meta_path + '/*/*')\n",
    "for i in img_abs_path[:1]:\n",
    "    img_rep = image_representation(img_abs_path=i, img_size=TARGET_SIZE, model=model_32, filters=filters_32, \n",
    "                                   thres=thres_32)\n",
    "    img_rep.exact_feature_maps(visualize=True)\n",
    "    act_idx = img_rep.exact_active_channels()\n",
    "    print('active index is', act_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of virtual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: optimize help layers\n",
    "# create bag of virtual words\n",
    "# help_layers = [2, 10, 22, 32, 42, 54, 64, 74, 84]\n",
    "help_layers = [10, 32, 42, 54]\n",
    "image_corpus = []\n",
    "for idx, row in df.iterrows():\n",
    "    words_all_layer = []\n",
    "    print('one image representation started')\n",
    "    print(row['img_abs_path'])\n",
    "    \n",
    "    for l in help_layers:\n",
    "        img_rep = image_representation(img_abs_path=row['img_abs_path'], img_size=TARGET_SIZE, model=vars()['model_' + str(l)], \n",
    "                                       filters=vars()['filters_' + str(l)], thres=vars()['thres_' + str(l)])\n",
    "        img_rep.exact_feature_maps(visualize=False)\n",
    "        img_rep.exact_active_channels()\n",
    "        words_per_layer = img_rep.create_bovw(abbv='filters_' + str(l))\n",
    "        words_all_layer+=words_per_layer\n",
    "    \n",
    "    # print('virtual words for this image is', words)\n",
    "    print('number of virtual words for this image is', len(words_all_layer))\n",
    "    print('one image representation finished')\n",
    "    \n",
    "    a = ' '.join(map(str, words_all_layer))\n",
    "    image_corpus.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', analyzer='word', lowercase=True)\n",
    "X_vect = vectorizer.fit_transform(image_corpus)\n",
    "vocab = tuple(vectorizer.get_feature_names())\n",
    "X = X_vect.toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine optimal topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: jaccard score looks wrong\n",
    "if False:\n",
    "    sim_scores = []\n",
    "    for n in N_TOPICS:\n",
    "        model = lda_model(X, image_corpus, vocab, n_topics=n, n_iter=1000)\n",
    "        model.fit()\n",
    "        sim_score = model.topic_path(df)\n",
    "        sim_scores.append(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    plt.figure()\n",
    "    plt.plot(N_TOPICS, sim_scores, label='Average Overlap Between Topics', linewidth=2, markersize=12)\n",
    "    plt.xlabel('Number of topics')\n",
    "    plt.ylabel('Average Jaccard similarity')   \n",
    "    plt.title('Average Jaccard Similarity Between Topics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = lda_model(X, image_corpus, vocab, n_topics=5, n_iter=1000)\n",
    "model.fit()\n",
    "sim_score = model.topic_path(df)\n",
    "model.topic_representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodel LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine image corpus with text\n",
    "df['img_attr'] = image_corpus\n",
    "multi_corpus = (df['attr_4'] + ' ' + df['img_attr']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', analyzer='word', lowercase=True)\n",
    "X_vect = vectorizer.fit_transform(multi_corpus)\n",
    "vocab = tuple(vectorizer.get_feature_names())\n",
    "X = X_vect.toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = lda_model(X, multi_corpus, vocab, n_topics=6, n_iter=1000)\n",
    "model.fit()\n",
    "sim_score = model.topic_path(df)\n",
    "model.topic_representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend top n assortment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_TOP_ASSORT = 7\n",
    "N_TOPCIS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_jpg'] = df['name'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for topic_no in range(N_TOPCIS):\n",
    "    keys = df.loc[df['topic'] == topic_no, 'name_jpg']\n",
    "    values = X[keys.index]\n",
    "    sim = cosine_similarity(values, values)\n",
    "    vars()['sim_matrix_' + str(topic_no)] = pd.DataFrame(sim, columns = keys, index = keys).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple search engine\n",
    "# top n assortment per image\n",
    "# img_name = df['name_jpg'].iloc[-40]\n",
    "img_name = df['name_jpg'].iloc[10]\n",
    "\n",
    "topic_no = df.loc[df['name_jpg'] == img_name, 'topic'].item()\n",
    "top_n_assort = vars()['sim_matrix_' + str(topic_no)].loc[img_name, :].sort_values(ascending = False).head(N_TOP_ASSORT)\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize = (80, 80))\n",
    "for name, sim_score in top_n_assort.iteritems():\n",
    "    try: \n",
    "        ax = plt.subplot(1, N_TOP_ASSORT, i+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        img_path = df.loc[df['name_jpg'] == name, 'img_abs_path'].item()\n",
    "        # plot filter channel in grayscale\n",
    "        plt.title('{}: \\n sim score is {:.2f}'.format(name, sim_score))\n",
    "        plt.imshow(load_img(img_path, target_size=(TARGET_SIZE, TARGET_SIZE)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create pickle files for recommender engine\n",
    "sim_names = []\n",
    "sim_values = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    topic_no = row['topic']\n",
    "    a = vars()['sim_matrix_' + str(topic_no)][row['name_jpg']].sort_values(ascending = False).head(N_TOP_ASSORT)\n",
    "    sim_name = a.index.tolist()\n",
    "    sim_value = a.values.tolist()\n",
    "    sim_names.append(sim_name)\n",
    "    sim_values.append(sim_value)\n",
    "\n",
    "sim_names = pd.DataFrame(sim_names, index=df['name_jpg'].values)\n",
    "sim_values = pd.DataFrame(sim_values, index=df['name_jpg'].values)\n",
    "sim_names.to_pickle(Path(static_model_path, 'similarNames.pkl'))\n",
    "sim_values.to_pickle(Path(static_model_path, 'similarValues.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create image dict and put it in ./Engine/imageRecommeder/commands/ in def importDB()\n",
    "images = []\n",
    "for i in df['name_jpg']:\n",
    "    img = {}\n",
    "    img['name'] = i\n",
    "    img['caption'] = img['name'].rsplit('.', 1)[0]\n",
    "    images.append(img)\n",
    "\n",
    "pickle.dump(images, open(Path(static_model_path, 'image_dict'), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove images existing in static folder and move all from meta.nosync there\n",
    "if not os.path.exists(static_image_path):\n",
    "    shutil.rmtree(static_image_path)\n",
    "    \n",
    "# create new dir\n",
    "if not os.path.exists(static_image_path):\n",
    "    os.makedirs(static_image_path)\n",
    "    \n",
    "for subdir in os.listdir(meta_path):\n",
    "    for file in os.listdir(Path(meta_path, subdir)):\n",
    "         shutil.copy(Path(meta_path, subdir, file), static_image_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "vaPYSd2kDHEq"
   ],
   "name": "furniture_image_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
